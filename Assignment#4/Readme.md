Assignment 4 : Practice colabs for Data Augmentation, Activation functions and Regularization.

a) Do a codelab with all hyperparameters learning rate decay, dropout, batch norm

Hint :https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist?hl=en#0

Submit the colab in github repository

b) Do various experiments with weights and biases of hyperparameters in weights and biases 

of various optimizers, layer depth width, learning rate etc.,. both in keras and pytorch(2 colabs) - submit your executed colab and artifactslinks.

 

https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb (Links to an external site.)

https://docs.wandb.ai/guides/sweeps

Hint :

Keras:

https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases_keras.ipynb (Links to an external site.)

 

Pytorch : 

https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb (Links to an external site.)

c) Do keras  classifier for regression problem, image problem submit your colabs and artifacts links

follow all best practices like normalization etc,.,

Hint ; image : fashionmnist or pets , regression : california housing, 

 

Hint : https://github.com/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb (Links to an external site.)

 

d) Write a colab to  Illustrate use of various activation functions, dropouts, learning rate schedulers , regularization techniques like gradient clipping, batch normalization, early stopping, l1 and l2 regularizations, optimizers on simple data set

Hint : https://github.com/ageron/handson-ml2/blob/master/11_training_deep_neural_networks.ipynb
