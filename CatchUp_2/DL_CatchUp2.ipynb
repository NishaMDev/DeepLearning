{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL-CatchUp2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPEfD3A4cZ+GBItS4P9Dgiy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NishaMDev/DeepLearning/blob/main/CatchUp_2/DL_CatchUp2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write colab to demonstrate in graph neural networks using pytorch geometric using both GCN and GAT layers."
      ],
      "metadata": {
        "id": "PBMt7Edinmg_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz5ULt4GnxIZ"
      },
      "source": [
        "In this tutorial, we will discuss the application of neural networks on graphs. Graph Neural Networks (GNNs) have recently gained increasing popularity in both applications and research, including domains such as social networks, knowledge graphs, recommender systems, and bioinformatics. While the theory and math behind GNNs might first seem complicated, the implementation of those models is quite simple and helps in understanding the methodology. Therefore, we will discuss the implementation of basic network layers of a GNN, namely graph convolutions, and attention layers. Finally, we will apply a GNN on a node-level, edge-level, and graph-level tasks.\n",
        "\n",
        "Below, we will start by importing our standard libraries. We will use PyTorch Lightning as already done in Tutorial 5 and 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HjtG2kjwnxIZ",
        "outputId": "640f6a5e-e31b-4182-db39-794a1bba8447",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np \n",
        "import time\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial7\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d0wy-NcnxIa"
      },
      "source": [
        "We also have a few pre-trained models we download below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sbwJhTT_nxIa"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL where saved models are stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/\"\n",
        "# Files to download\n",
        "pretrained_files = [\"NodeLevelMLP.ckpt\", \"NodeLevelGNN.ckpt\", \"GraphLevelGraphConv.ckpt\"]\n",
        "\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if \"/\" in file_name:\n",
        "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ivCahFVnxIb"
      },
      "source": [
        "## Graph Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8G12W-onxIb"
      },
      "source": [
        "### Graph representation\n",
        "\n",
        "Before starting the discussion of specific neural network operations on graphs, we should consider how to represent a graph. Mathematically, a graph $\\mathcal{G}$ is defined as a tuple of a set of nodes/vertices $V$, and a set of edges/links $E$: $\\mathcal{G}=(V,E)$. Each edge is a pair of two vertices, and represents a connection between them. For instance, let's look at the following graph:\n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"><img src=\"example_graph.svg\" width=\"250px\"></center>\n",
        "\n",
        "The vertices are $V=\\{1,2,3,4\\}$, and edges $E=\\{(1,2), (2,3), (2,4), (3,4)\\}$. Note that for simplicity, we assume the graph to be undirected and hence don't add mirrored pairs like $(2,1)$. In application, vertices and edge can often have specific attributes, and edges can even be directed. The question is how we could represent this diversity in an efficient way for matrix operations. Usually, for the edges, we decide between two variants: an adjacency matrix, or a list of paired vertex indices. \n",
        "\n",
        "The **adjacency matrix** $A$ is a square matrix whose elements indicate whether pairs of vertices are adjacent, i.e. connected, or not. In the simplest case, $A_{ij}$ is 1 if there is a connection from node $i$ to $j$, and otherwise 0. If we have edge attributes or different categories of edges in a graph, this information can be added to the matrix as well. For an undirected graph, keep in mind that $A$ is a symmetric matrix ($A_{ij}=A_{ji}$). For the example graph above, we have the following adjacency matrix:\n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix}\n",
        "    0 & 1 & 0 & 0\\\\\n",
        "    1 & 0 & 1 & 1\\\\\n",
        "    0 & 1 & 0 & 1\\\\\n",
        "    0 & 1 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "While expressing a graph as a list of edges is more efficient in terms of memory and (possibly) computation, using an adjacency matrix is more intuitive and simpler to implement. In our implementations below, we will rely on the adjacency matrix to keep the code simple. However, common libraries use edge lists, which we will discuss later more.\n",
        "Alternatively, we could also use the list of edges to define a sparse adjacency matrix with which we can work as if it was a dense matrix, but allows more memory-efficient operations. PyTorch supports this with the sub-package `torch.sparse` ([documentation](https://pytorch.org/docs/stable/sparse.html)) which is however still in a beta-stage (API might change in future)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mrea1PznxIc"
      },
      "source": [
        "### Graph Convolutions\n",
        "\n",
        "Graph Convolutional Networks have been introduced by [Kipf et al.](https://openreview.net/pdf?id=SJU4ayYgl) in 2016 at the University of Amsterdam. He also wrote a great [blog post](https://tkipf.github.io/graph-convolutional-networks/) about this topic, which is recommended if you want to read about GCNs from a different perspective. GCNs are similar to convolutions in images in the sense that the \"filter\" parameters are typically shared over all locations in the graph. At the same time, GCNs rely on message passing methods, which means that vertices exchange information with the neighbors, and send \"messages\" to each other. Before looking at the math, we can try to visually understand how GCNs work. The first step is that each node creates a feature vector that represents the message it wants to send to all its neighbors. In the second step, the messages are sent to the neighbors, so that a node receives one message per adjacent node. Below we have visualized the two steps for our example graph. \n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"><img src=\"graph_message_passing.svg\" width=\"700px\"></center>\n",
        "\n",
        "If we want to formulate that in more mathematical terms, we need to first decide how to combine all the messages a node receives. As the number of messages vary across nodes, we need an operation that works for any number. Hence, the usual way to go is to sum or take the mean. Given the previous features of nodes $H^{(l)}$, the GCN layer is defined as follows:\n",
        "\n",
        "$$H^{(l+1)} = \\sigma\\left(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{(l)}W^{(l)}\\right)$$\n",
        "\n",
        "$W^{(l)}$ is the weight parameters with which we transform the input features into messages ($H^{(l)}W^{(l)}$). To the adjacency matrix $A$ we add the identity matrix so that each node sends its own message also to itself: $\\hat{A}=A+I$. Finally, to take the average instead of summing, we calculate the matrix $\\hat{D}$ which is a diagonal matrix with $D_{ii}$ denoting the number of neighbors node $i$ has. $\\sigma$ represents an arbitrary activation function, and not necessarily the sigmoid (usually a ReLU-based activation function is used in GNNs). \n",
        "\n",
        "When implementing the GCN layer in PyTorch, we can take advantage of the flexible operations on tensors. Instead of defining a matrix $\\hat{D}$, we can simply divide the summed messages by the number of neighbors afterward. Additionally, we replace the weight matrix with a linear layer, which additionally allows us to add a bias. Written as a PyTorch module, the GCN layer is defined as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CaacqDU6nxIc"
      },
      "outputs": [],
      "source": [
        "class GCNLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_in, c_out):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(c_in, c_out)\n",
        "\n",
        "    def forward(self, node_feats, adj_matrix):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
        "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
        "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections. \n",
        "                         Shape: [batch_size, num_nodes, num_nodes]\n",
        "        \"\"\"\n",
        "        # Num neighbours = number of incoming edges\n",
        "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
        "        node_feats = self.projection(node_feats)\n",
        "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
        "        node_feats = node_feats / num_neighbours\n",
        "        return node_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYNpETQFnxIc"
      },
      "source": [
        "To further understand the GCN layer, we can apply it to our example graph above. First, let's specify some node features and the adjacency matrix with added self-connections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CdlyNLF2nxIc",
        "outputId": "ed1fb768-53f8-442b-d359-d5c7a60cc1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node features:\n",
            " tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "\n",
            "Adjacency matrix:\n",
            " tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n"
          ]
        }
      ],
      "source": [
        "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
        "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
        "                            [1, 1, 1, 1],\n",
        "                            [0, 1, 1, 1],\n",
        "                            [0, 1, 1, 1]]])\n",
        "\n",
        "print(\"Node features:\\n\", node_feats)\n",
        "print(\"\\nAdjacency matrix:\\n\", adj_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdMC8T0mnxId"
      },
      "source": [
        "Next, let's apply a GCN layer to it. For simplicity, we initialize the linear weight matrix as an identity matrix so that the input features are equal to the messages. This makes it easier for us to verify the message passing operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6kJbRqp7nxId",
        "outputId": "6d99de12-117f-432f-806b-81232bc05b85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n",
            "Input features tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "Output features tensor([[[1., 2.],\n",
            "         [3., 4.],\n",
            "         [4., 5.],\n",
            "         [4., 5.]]])\n"
          ]
        }
      ],
      "source": [
        "layer = GCNLayer(c_in=2, c_out=2)\n",
        "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
        "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_feats = layer(node_feats, adj_matrix)\n",
        "\n",
        "print(\"Adjacency matrix\", adj_matrix)\n",
        "print(\"Input features\", node_feats)\n",
        "print(\"Output features\", out_feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eD5BVUXnxId"
      },
      "source": [
        "As we can see, the first node's output values are the average of itself and the second node. Similarly, we can verify all other nodes. However, in a GNN, we would also want to allow feature exchange between nodes beyond its neighbors. This can be achieved by applying multiple GCN layers, which gives us the final layout of a GNN. The GNN can be build up by a sequence of GCN layers and non-linearities such as ReLU. For a visualization, see below (figure credit - [Thomas Kipf, 2016](https://tkipf.github.io/graph-convolutional-networks/)).\n",
        "\n",
        "<center width=\"100%\" style=\"padding: 10px\"><img src=\"gcn_network.png\" width=\"600px\"></center>\n",
        "\n",
        "However, one issue we can see from looking at the example above is that the output features for nodes 3 and 4 are the same because they have the same adjacent nodes (including itself). Therefore, GCN layers can make the network forget node-specific information if we just take a mean over all messages. Multiple possible improvements have been proposed. While the simplest option might be using residual connections, the more common approach is to either weigh the self-connections higher or define a separate weight matrix for the self-connections. Alternatively, we can re-visit a concept from the last tutorial: attention. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRbSqcXwnxId"
      },
      "source": [
        "### Graph Attention \n",
        "\n",
        "If you remember from the last tutorial, attention describes a weighted average of multiple elements with the weights dynamically computed based on an input query and elements' keys (if you haven't read Tutorial 6 yet, it is recommended to at least go through the very first section called [What is Attention?](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html#What-is-Attention?)). This concept can be similarly applied to graphs, one of such is the Graph Attention Network (called GAT, proposed by [Velickovic et al., 2017](https://arxiv.org/abs/1710.10903)). Similarly to the GCN, the graph attention layer creates a message for each node using a linear layer/weight matrix. For the attention part, it uses the message from the node itself as a query, and the messages to average as both keys and values (note that this also includes the message to itself). The score function $f_{attn}$ is implemented as a one-layer MLP which maps the query and key to a single value. The MLP looks as follows (figure credit - [Velickovic et al.](https://arxiv.org/abs/1710.10903)):\n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"><img src=\"graph_attention_MLP.svg\" width=\"250px\"></center>\n",
        "\n",
        "$h_i$ and $h_j$ are the original features from node $i$ and $j$ respectively, and represent the messages of the layer with $\\mathbf{W}$ as weight matrix. $\\mathbf{a}$ is the weight matrix of the MLP, which has the shape $[1,2\\times d_{\\text{message}}]$, and $\\alpha_{ij}$ the final attention weight from node $i$ to $j$. The calculation can be described as follows:\n",
        "\n",
        "$$\\alpha_{ij} = \\frac{\\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_j\\right]\\right)\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_k\\right]\\right)\\right)}$$\n",
        "\n",
        "The operator $||$ represents the concatenation, and $\\mathcal{N}_i$ the indices of the neighbors of node $i$. Note that in contrast to usual practice, we apply a non-linearity (here LeakyReLU) before the softmax over elements. Although it seems like a minor change at first, it is crucial for the attention to depend on the original input. Specifically, let's remove the non-linearity for a second, and try to simplify the expression:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\alpha_{ij} & = \\frac{\\exp\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_j\\right]\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_k\\right]\\right)}\\\\[5pt]\n",
        "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i+\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i+\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\[5pt]\n",
        "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i\\right)\\cdot\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i\\right)\\cdot\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\[5pt]\n",
        "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "We can see that without the non-linearity, the attention term with $h_i$ actually cancels itself out, resulting in the attention being independent of the node itself. Hence, we would have the same issue as the GCN of creating the same output features for nodes with the same neighbors. This is why the LeakyReLU is crucial and adds some dependency on $h_i$ to the attention. \n",
        "\n",
        "Once we obtain all attention factors, we can calculate the output features for each node by performing the weighted average:\n",
        "\n",
        "$$h_i'=\\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}\\mathbf{W}h_j\\right)$$\n",
        "\n",
        "$\\sigma$ is yet another non-linearity, as in the GCN layer. Visually, we can represent the full message passing in an attention layer as follows (figure credit - [Velickovic et al.](https://arxiv.org/abs/1710.10903)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"graph_attention.jpeg\" width=\"400px\"></center>\n",
        "\n",
        "To increase the expressiveness of the graph attention network, [Velickovic et al.](https://arxiv.org/abs/1710.10903) proposed to extend it to multiple heads similar to the Multi-Head Attention block in Transformers. This results in $N$ attention layers being applied in parallel. In the image above, it is visualized as three different colors of arrows (green, blue, and purple) that are afterward concatenated. The average is only applied for the very final prediction layer in a network. \n",
        "\n",
        "After having discussed the graph attention layer in detail, we can implement it below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mqsoa0Q3nxId"
      },
      "outputs": [],
      "source": [
        "class GATLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimensionality of input features\n",
        "            c_out - Dimensionality of output features\n",
        "            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The \n",
        "                        output features are equally split up over the heads if concat_heads=True.\n",
        "            concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
        "            alpha - Negative slope of the LeakyReLU activation.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.concat_heads = concat_heads\n",
        "        if self.concat_heads:\n",
        "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
        "            c_out = c_out // num_heads\n",
        "        \n",
        "        # Sub-modules and parameters needed in the layer\n",
        "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
        "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out)) # One per head\n",
        "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
        "        \n",
        "        # Initialization from the original implementation\n",
        "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "        \n",
        "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
        "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
        "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
        "        \"\"\"\n",
        "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
        "        \n",
        "        # Apply linear layer and sort nodes by head\n",
        "        node_feats = self.projection(node_feats)\n",
        "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
        "        \n",
        "        # We need to calculate the attention logits for every edge in the adjacency matrix \n",
        "        # Doing this on all possible combinations of nodes is very expensive\n",
        "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
        "        edges = adj_matrix.nonzero(as_tuple=False) # Returns indices where the adjacency matrix is not 0 => edges\n",
        "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
        "        edge_indices_row = edges[:,0] * num_nodes + edges[:,1]\n",
        "        edge_indices_col = edges[:,0] * num_nodes + edges[:,2]\n",
        "        a_input = torch.cat([\n",
        "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
        "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
        "        ], dim=-1) # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
        "        \n",
        "        # Calculate attention MLP output (independent for each head)\n",
        "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a) \n",
        "        attn_logits = self.leakyrelu(attn_logits)\n",
        "        \n",
        "        # Map list of attention values back into a matrix\n",
        "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
        "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
        "        \n",
        "        # Weighted average of attention\n",
        "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
        "        if print_attn_probs:\n",
        "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
        "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
        "        \n",
        "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
        "        if self.concat_heads:\n",
        "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
        "        else:\n",
        "            node_feats = node_feats.mean(dim=2)\n",
        "        \n",
        "        return node_feats "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqXqs8jpnxIe"
      },
      "source": [
        "Again, we can apply the graph attention layer on our example graph above to understand the dynamics better. As before, the input layer is initialized as an identity matrix, but we set $\\mathbf{a}$ to be a vector of arbitrary numbers to obtain different attention values. We use two heads to show the parallel, independent attention mechanisms working in the layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OtldNqGdnxIe",
        "outputId": "65e9edbd-2fbf-473a-c008-14814faed20c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention probs\n",
            " tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
            "          [0.1096, 0.1450, 0.2642, 0.4813],\n",
            "          [0.0000, 0.1858, 0.2885, 0.5257],\n",
            "          [0.0000, 0.2391, 0.2696, 0.4913]],\n",
            "\n",
            "         [[0.5100, 0.4900, 0.0000, 0.0000],\n",
            "          [0.2975, 0.2436, 0.2340, 0.2249],\n",
            "          [0.0000, 0.3838, 0.3142, 0.3019],\n",
            "          [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
            "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n",
            "Input features tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "Output features tensor([[[1.2913, 1.9800],\n",
            "         [4.2344, 3.7725],\n",
            "         [4.6798, 4.8362],\n",
            "         [4.5043, 4.7351]]])\n"
          ]
        }
      ],
      "source": [
        "layer = GATLayer(2, 2, num_heads=2)\n",
        "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
        "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
        "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.1, -0.1]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_feats = layer(node_feats, adj_matrix, print_attn_probs=True)\n",
        "\n",
        "print(\"Adjacency matrix\", adj_matrix)\n",
        "print(\"Input features\", node_feats)\n",
        "print(\"Output features\", out_feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBuBncNznxIe"
      },
      "source": [
        "We recommend that you try to calculate the attention matrix at least for one head and one node for yourself. The entries are 0 where there does not exist an edge between $i$ and $j$. For the others, we see a diverse set of attention probabilities. Moreover, the output features of node 3 and 4 are now different although they have the same neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2dG0NnLnxIe"
      },
      "source": [
        "## PyTorch Geometric\n",
        "\n",
        "We had mentioned before that implementing graph networks with adjacency matrix is simple and straight-forward but can be computationally expensive for large graphs. Many real-world graphs can reach over 200k nodes, for which adjacency matrix-based implementations fail. There are a lot of optimizations possible when implementing GNNs, and luckily, there exist packages that provide such layers. The most popular packages for PyTorch are [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) and the [Deep Graph Library](https://www.dgl.ai/) (the latter being actually framework agnostic). Which one to use depends on the project you are planning to do and personal taste. In this tutorial, we will look at PyTorch Geometric as part of the PyTorch family. Similar to PyTorch Lightning, PyTorch Geometric is not installed by default on GoogleColab (and actually also not in our `dl2021` environment due to many dependencies that would be unnecessary for the practicals). Hence, let's import and/or install it below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CTf2UvcanxIe"
      },
      "outputs": [],
      "source": [
        "# torch geometric\n",
        "try: \n",
        "    import torch_geometric\n",
        "except ModuleNotFoundError:\n",
        "    # Installing torch geometric packages with specific CUDA+PyTorch version. \n",
        "    # See https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html for details \n",
        "    TORCH = torch.__version__.split('+')[0]\n",
        "    CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "\n",
        "    !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-geometric \n",
        "    import torch_geometric\n",
        "import torch_geometric.nn as geom_nn\n",
        "import torch_geometric.data as geom_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQcpraT5nxIe"
      },
      "source": [
        "PyTorch Geometric provides us a set of common graph layers, including the GCN and GAT layer we implemented above. Additionally, similar to PyTorch's torchvision, it provides the common graph datasets and transformations on those to simplify training. Compared to our implementation above, PyTorch Geometric uses a list of index pairs to represent the edges. The details of this library will be explored further in our experiments.\n",
        "\n",
        "In our tasks below, we want to allow us to pick from a multitude of graph layers. Thus, we define again below a dictionary to access those using a string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ehyF8eZEnxIe"
      },
      "outputs": [],
      "source": [
        "gnn_layer_by_name = {\n",
        "    \"GCN\": geom_nn.GCNConv,\n",
        "    \"GAT\": geom_nn.GATConv,\n",
        "    \"GraphConv\": geom_nn.GraphConv\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QENXtJqnxIe"
      },
      "source": [
        "Additionally to GCN and GAT, we added the layer `geom_nn.GraphConv` ([documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GraphConv)). GraphConv is a GCN with a separate weight matrix for the self-connections. Mathematically, this would be:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_i^{(l+1)} = \\mathbf{W}^{(l + 1)}_1 \\mathbf{x}_i^{(l)} + \\mathbf{W}^{(\\ell + 1)}_2 \\sum_{j \\in \\mathcal{N}_i} \\mathbf{x}_j^{(l)}\n",
        "$$\n",
        "\n",
        "In this formula, the neighbor's messages are added instead of averaged. However, PyTorch Geometric provides the argument `aggr` to switch between summing, averaging, and max pooling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba3LKoISnxIe"
      },
      "source": [
        "## Experiments on graph structures\n",
        "\n",
        "Tasks on graph-structured data can be grouped into three groups: node-level, edge-level and graph-level. The different levels describe on which level we want to perform classification/regression. We will discuss all three types in more detail below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A - Node Classification Problem"
      ],
      "metadata": {
        "id": "QRWuKp0Duo2q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoeRjgPTnxIe"
      },
      "source": [
        "### Node-level tasks: Semi-supervised node classification\n",
        "\n",
        "Node-level tasks have the goal to classify nodes in a graph. Usually, we have given a single, large graph with >1000 nodes of which a certain amount of nodes are labeled. We learn to classify those labeled examples during training and try to generalize to the unlabeled nodes. \n",
        "\n",
        "A popular example that we will use in this tutorial is the Cora dataset, a citation network among papers. The Cora consists of 2708 scientific publications with links between each other representing the citation of one paper by another. The task is to classify each publication into one of seven classes. Each publication is represented by a bag-of-words vector. This means that we have a vector of 1433 elements for each publication, where a 1 at feature $i$ indicates that the $i$-th word of a pre-defined dictionary is in the article. Binary bag-of-words representations are commonly used when we need very simple encodings, and already have an intuition of what words to expect in a network. There exist much better approaches, but we will leave this to the NLP courses to discuss.\n",
        "\n",
        "We will load the dataset below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R_iNI5NcnxIf"
      },
      "outputs": [],
      "source": [
        "cora_dataset = torch_geometric.datasets.Planetoid(root=DATASET_PATH, name=\"Cora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deGeQTCtnxIf"
      },
      "source": [
        "Let's look at how PyTorch Geometric represents the graph data. Note that although we have a single graph, PyTorch Geometric returns a dataset for compatibility to other datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w-79xfURnxIf",
        "outputId": "718d0a49-b3e7-4938-a458-8b53a211f0a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "cora_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoacMD6xnxIf"
      },
      "source": [
        "The graph is represented by a `Data` object ([documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data)) which we can access as a standard Python namespace. The edge index tensor is the list of edges in the graph and contains the mirrored version of each edge for undirected graphs. The `train_mask`, `val_mask`, and `test_mask` are boolean masks that indicate which nodes we should use for training, validation, and testing. The `x` tensor is the feature tensor of our 2708 publications, and `y` the labels for all nodes.\n",
        "\n",
        "After having seen the data, we can implement a simple graph neural network. The GNN applies a sequence of graph layers (GCN, GAT, or GraphConv), ReLU as activation function, and dropout for regularization. See below for the specific implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oZ377sJgnxIf"
      },
      "outputs": [],
      "source": [
        "class GNNModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GCN\", dp_rate=0.1, **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimension of input features\n",
        "            c_hidden - Dimension of hidden features\n",
        "            c_out - Dimension of the output features. Usually number of classes in classification\n",
        "            num_layers - Number of \"hidden\" graph layers\n",
        "            layer_name - String of the graph layer to use\n",
        "            dp_rate - Dropout rate to apply throughout the network\n",
        "            kwargs - Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        gnn_layer = gnn_layer_by_name[layer_name]\n",
        "        \n",
        "        layers = []\n",
        "        in_channels, out_channels = c_in, c_hidden\n",
        "        for l_idx in range(num_layers-1):\n",
        "            layers += [\n",
        "                gnn_layer(in_channels=in_channels, \n",
        "                          out_channels=out_channels,\n",
        "                          **kwargs),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dp_rate)\n",
        "            ]\n",
        "            in_channels = c_hidden\n",
        "        layers += [gnn_layer(in_channels=in_channels, \n",
        "                             out_channels=c_out,\n",
        "                             **kwargs)]\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "    \n",
        "    def forward(self, x, edge_index):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features per node\n",
        "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
        "        \"\"\"\n",
        "        for l in self.layers:\n",
        "            # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
        "            # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
        "            # we can simply check the class type.\n",
        "            if isinstance(l, geom_nn.MessagePassing):\n",
        "                x = l(x, edge_index)\n",
        "            else:\n",
        "                x = l(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcEOT8o3nxIf"
      },
      "source": [
        "Good practice in node-level tasks is to create an MLP baseline that is applied to each node independently. This way we can verify whether adding the graph information to the model indeed improves the prediction, or not. It might also be that the features per node are already expressive enough to clearly point towards a specific class. To check this, we implement a simple MLP below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CVB6PKgmnxIf"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimension of input features\n",
        "            c_hidden - Dimension of hidden features\n",
        "            c_out - Dimension of the output features. Usually number of classes in classification\n",
        "            num_layers - Number of hidden layers\n",
        "            dp_rate - Dropout rate to apply throughout the network\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        in_channels, out_channels = c_in, c_hidden\n",
        "        for l_idx in range(num_layers-1):\n",
        "            layers += [\n",
        "                nn.Linear(in_channels, out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dp_rate)\n",
        "            ]\n",
        "            in_channels = c_hidden\n",
        "        layers += [nn.Linear(in_channels, c_out)]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features per node\n",
        "        \"\"\"\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja8rxuoqnxIf"
      },
      "source": [
        "Finally, we can merge the models into a PyTorch Lightning module which handles the training, validation, and testing for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "N4ToYDYenxIf"
      },
      "outputs": [],
      "source": [
        "class NodeLevelGNN(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, model_name, **model_kwargs):\n",
        "        super().__init__()\n",
        "        # Saving hyperparameters\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        if model_name == \"MLP\":\n",
        "            self.model = MLPModel(**model_kwargs)\n",
        "        else:\n",
        "            self.model = GNNModel(**model_kwargs)\n",
        "        self.loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, data, mode=\"train\"):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.model(x, edge_index)\n",
        "        \n",
        "        # Only calculate the loss on the nodes corresponding to the mask\n",
        "        if mode == \"train\":\n",
        "            mask = data.train_mask\n",
        "        elif mode == \"val\":\n",
        "            mask = data.val_mask\n",
        "        elif mode == \"test\":\n",
        "            mask = data.test_mask\n",
        "        else:\n",
        "            assert False, f\"Unknown forward mode: {mode}\"\n",
        "        \n",
        "        loss = self.loss_module(x[mask], data.y[mask])\n",
        "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
        "        return loss, acc\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # We use SGD here, but Adam works as well \n",
        "        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, acc = self.forward(batch, mode=\"train\")\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"val\")\n",
        "        self.log('val_acc', acc)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"test\")\n",
        "        self.log('test_acc', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryQrfGw1nxIf"
      },
      "source": [
        "Additionally to the Lightning module, we define a training function below. As we have a single graph, we use a batch size of 1 for the data loader and share the same data loader for the train, validation, and test set (the mask is picked inside the Lightning module). Besides, we set the argument `progress_bar_refresh_rate` to zero as it usually shows the progress per epoch, but an epoch only consists of a single step. The rest of the code is very similar to what we have seen in Tutorial 5 and 6 already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J89S92aCnxIf"
      },
      "outputs": [],
      "source": [
        "def train_node_classifier(model_name, dataset, **model_kwargs):\n",
        "    pl.seed_everything(42)\n",
        "    node_data_loader = geom_data.DataLoader(dataset, batch_size=1)\n",
        "    \n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"NodeLevel\" + model_name)\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         gpus=1 if str(device).startswith(\"cuda\") else 0,\n",
        "                         max_epochs=200,\n",
        "                         progress_bar_refresh_rate=0) # 0 because epoch size is 1\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"NodeLevel{model_name}.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = NodeLevelGNN.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything()\n",
        "        model = NodeLevelGNN(model_name=model_name, c_in=dataset.num_node_features, c_out=dataset.num_classes, **model_kwargs)\n",
        "        trainer.fit(model, node_data_loader, node_data_loader)\n",
        "        model = NodeLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "    \n",
        "    # Test best model on the test set\n",
        "    test_result = trainer.test(model, node_data_loader, verbose=False)\n",
        "    batch = next(iter(node_data_loader))\n",
        "    batch = batch.to(model.device)\n",
        "    _, train_acc = model.forward(batch, mode=\"train\")\n",
        "    _, val_acc = model.forward(batch, mode=\"val\")\n",
        "    result = {\"train\": train_acc,\n",
        "              \"val\": val_acc,\n",
        "              \"test\": test_result[0]['test_acc']}\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOeAmfQInxIf"
      },
      "source": [
        "Finally, we can train our models. First, let's train the simple MLP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "piFDpCEVnxIf"
      },
      "outputs": [],
      "source": [
        "# Small function for printing the test scores\n",
        "def print_results(result_dict):\n",
        "    if \"train\" in result_dict:\n",
        "        print(f\"Train accuracy: {(100.0*result_dict['train']):4.2f}%\")\n",
        "    if \"val\" in result_dict:\n",
        "        print(f\"Val accuracy:   {(100.0*result_dict['val']):4.2f}%\")\n",
        "    print(f\"Test accuracy:  {(100.0*result_dict['test']):4.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "w446ARIrnxIf",
        "outputId": "8fef3e12-41ae-4f85-91a0-7a012f39fbcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 42\n",
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:97: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found pretrained model, loading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  category=PossibleUserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:73: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2708. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 97.14%\n",
            "Val accuracy:   54.60%\n",
            "Test accuracy:  60.60%\n"
          ]
        }
      ],
      "source": [
        "node_mlp_model, node_mlp_result = train_node_classifier(model_name=\"MLP\",\n",
        "                                                        dataset=cora_dataset,\n",
        "                                                        c_hidden=16,\n",
        "                                                        num_layers=2,\n",
        "                                                        dp_rate=0.1)\n",
        "\n",
        "print_results(node_mlp_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI2lOo4ZnxIg"
      },
      "source": [
        "Although the MLP can overfit on the training dataset because of the high-dimensional input features, it does not perform too well on the test set. Let's see if we can beat this score with our graph networks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EDZwnP_YnxIg",
        "outputId": "94291ac9-60bd-46a0-e702-a2de0a8a4fd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 42\n",
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:97: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found pretrained model, loading...\n",
            "Train accuracy: 100.00%\n",
            "Val accuracy:   78.60%\n",
            "Test accuracy:  82.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  category=PossibleUserWarning,\n"
          ]
        }
      ],
      "source": [
        "node_gnn_model, node_gnn_result = train_node_classifier(model_name=\"GNN\",\n",
        "                                                        layer_name=\"GCN\",\n",
        "                                                        dataset=cora_dataset, \n",
        "                                                        c_hidden=16, \n",
        "                                                        num_layers=2,\n",
        "                                                        dp_rate=0.1)\n",
        "print_results(node_gnn_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdO0Jyr2nxIg"
      },
      "source": [
        "As we would have hoped for, the GNN model outperforms the MLP by quite a margin. This shows that using the graph information indeed improves our predictions and lets us generalizes better.\n",
        "\n",
        "The hyperparameters in the model have been chosen to create a relatively small network. This is because the first layer with an input dimension of 1433 can be relatively expensive to perform for large graphs. In general, GNNs can become relatively expensive for very big graphs. This is why such GNNs either have a small hidden size or use a special batching strategy where we sample a connected subgraph of the big, original graph. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B - Edge Prediction Problem"
      ],
      "metadata": {
        "id": "EYnrTBfRuuo9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eQTjlaInxIg"
      },
      "source": [
        "### Edge-level tasks: Link prediction\n",
        "\n",
        "In some applications, we might have to predict on an edge-level instead of node-level. The most common edge-level task in GNN is link prediction. Link prediction means that given a graph, we want to predict whether there will be/should be an edge between two nodes or not. For example, in a social network, this is used by Facebook and co to propose new friends to you. Again, graph level information can be crucial to perform this task. The output prediction is usually done by performing a similarity metric on the pair of node features, which should be 1 if there should be a link, and otherwise close to 0. To keep the tutorial short, we will not implement this task ourselves. Nevertheless, there are many good resources out there if you are interested in looking closer at this task.\n",
        "Tutorials and papers for this topic include:\n",
        "\n",
        "* [PyTorch Geometric example](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/link_pred.py)\n",
        "* [Graph Neural Networks: A Review of Methods and Applications](https://arxiv.org/pdf/1812.08434.pdf), Zhou et al. 2019\n",
        "* [Link Prediction Based on Graph Neural Networks](https://papers.nips.cc/paper/2018/file/53f0d7c537d99b3824f0f99d62ea2428-Paper.pdf), Zhang and Chen, 2018."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8XYo7lkvQBX"
      },
      "source": [
        "In this example, we use our implementation of the [GCN](https://arxiv.org/abs/1609.02907) algorithm to build a model that predicts citation links in the Cora dataset (see below). The problem is treated as a supervised link prediction problem on a homogeneous citation network with nodes representing papers (with attributes such as binary keyword indicators and categorical subject) and links corresponding to paper-paper citations. \n",
        "\n",
        "To address this problem, we build a model with the following architecture. First we build a two-layer GCN model that takes labeled node pairs (`citing-paper` -> `cited-paper`)  corresponding to possible citation links, and outputs a pair of node embeddings for the `citing-paper` and `cited-paper` nodes of the pair. These embeddings are then fed into a link classification layer, which first applies a binary operator to those node embeddings (e.g., concatenating them) to construct the embedding of the potential link. Thus obtained link embeddings are passed through the dense link classification layer to obtain link predictions - probability for these candidate links to actually exist in the network. The entire model is trained end-to-end by minimizing the loss function of choice (e.g., binary cross-entropy between predicted link probabilities and true link labels, with true/false citation links having labels 1/0) using stochastic gradient descent (SGD) updates of the model parameters, with minibatches of 'training' links fed into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "nbsphinx": "hidden",
        "tags": [
          "CloudRunner"
        ],
        "id": "4822fnZBvQBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b2ebb38-3430-43a2-bf0f-ab463495006f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     || 435 kB 4.9 MB/s \n",
            "\u001b[K     || 482 kB 57.2 MB/s \n",
            "\u001b[K     || 462 kB 56.2 MB/s \n",
            "\u001b[K     || 41 kB 556 kB/s \n",
            "\u001b[?25h  Building wheel for mplleaflet (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# install StellarGraph if running on Google Colab\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  %pip install -q stellargraph[demos]==1.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "nbsphinx": "hidden",
        "tags": [
          "VersionCheck"
        ],
        "id": "zxZ4cQ97vQBY"
      },
      "outputs": [],
      "source": [
        "# verify that we're using the correct version of StellarGraph for this notebook\n",
        "import stellargraph as sg\n",
        "\n",
        "try:\n",
        "    sg.utils.validate_notebook_version(\"1.2.1\")\n",
        "except AttributeError:\n",
        "    raise ValueError(\n",
        "        f\"This notebook requires StellarGraph version 1.2.1, but a different version {sg.__version__} is installed.  Please see <https://github.com/stellargraph/stellargraph/issues/1172>.\"\n",
        "    ) from None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gQfHu6gTvQBY"
      },
      "outputs": [],
      "source": [
        "import stellargraph as sg\n",
        "from stellargraph.data import EdgeSplitter\n",
        "from stellargraph.mapper import FullBatchLinkGenerator\n",
        "from stellargraph.layer import GCN, LinkEmbedding\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from sklearn import preprocessing, feature_extraction, model_selection\n",
        "\n",
        "from stellargraph import globalvar\n",
        "from stellargraph import datasets\n",
        "from IPython.display import display, HTML\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnAwHnduvQBY"
      },
      "source": [
        "### Loading the CORA network data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "DataLoadingLinks"
        ],
        "id": "8gTqFgaEvQBZ"
      },
      "source": [
        "(See [the \"Loading from Pandas\" demo](../basics/loading-pandas.ipynb) for details on how data can be loaded.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "tags": [
          "DataLoading"
        ],
        "id": "90j8fviyvQBZ",
        "outputId": "1eb03872-48a6-49f2-8245-9e94ca7902fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset = datasets.Cora()\n",
        "display(HTML(dataset.description))\n",
        "G, _ = dataset.load(subject_as_feature=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "p7s0YB2JvQBa",
        "outputId": "cde1d8f3-e913-4461-854c-306913371629",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StellarGraph: Undirected multigraph\n",
            " Nodes: 2708, Edges: 5429\n",
            "\n",
            " Node types:\n",
            "  paper: [2708]\n",
            "    Features: float32 vector, length 1440\n",
            "    Edge types: paper-cites->paper\n",
            "\n",
            " Edge types:\n",
            "    paper-cites->paper: [5429]\n",
            "        Weights: all 1 (default)\n",
            "        Features: none\n"
          ]
        }
      ],
      "source": [
        "print(G.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv0wH7lJvQBa"
      },
      "source": [
        "We aim to train a link prediction model, hence we need to prepare the train and test sets of links and the corresponding graphs with those links removed.\n",
        "\n",
        "We are going to split our input graph into a train and test graphs using the EdgeSplitter class in `stellargraph.data`. We will use the train graph for training the model (a binary classifier that, given two nodes, predicts whether a link between these two nodes should exist or not) and the test graph for evaluating the model's performance on hold out data.\n",
        "Each of these graphs will have the same number of nodes as the input graph, but the number of links will differ (be reduced) as some of the links will be removed during each split and used as the positive samples for training/testing the link prediction classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M10qOfNZvQBa"
      },
      "source": [
        "From the original graph G, extract a randomly sampled subset of test edges (true and false citation links) and the reduced graph G_test with the positive test edges removed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-2-idHGwvQBa",
        "outputId": "d2f3f4b6-03d8-49a1-b066-14679f84e8c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Sampled 542 positive and 542 negative edges. **\n"
          ]
        }
      ],
      "source": [
        "# Define an edge splitter on the original graph G:\n",
        "edge_splitter_test = EdgeSplitter(G)\n",
        "\n",
        "# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G, and obtain the\n",
        "# reduced graph G_test with the sampled links removed:\n",
        "G_test, edge_ids_test, edge_labels_test = edge_splitter_test.train_test_split(\n",
        "    p=0.1, method=\"global\", keep_connected=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPAXD74ovQBb"
      },
      "source": [
        "The reduced graph G_test, together with the test ground truth set of links (edge_ids_test, edge_labels_test), will be used for testing the model.\n",
        "\n",
        "Now repeat this procedure to obtain the training data for the model. From the reduced graph G_test, extract a randomly sampled subset of train edges (true and false citation links) and the reduced graph G_train with the positive train edges removed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DeV9VnwYvQBb",
        "outputId": "1a1aea40-3fe3-4f84-a324-6327cf75a0ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Sampled 488 positive and 488 negative edges. **\n"
          ]
        }
      ],
      "source": [
        "# Define an edge splitter on the reduced graph G_test:\n",
        "edge_splitter_train = EdgeSplitter(G_test)\n",
        "\n",
        "# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G_test, and obtain the\n",
        "# reduced graph G_train with the sampled links removed:\n",
        "G_train, edge_ids_train, edge_labels_train = edge_splitter_train.train_test_split(\n",
        "    p=0.1, method=\"global\", keep_connected=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ-PKGwGvQBb"
      },
      "source": [
        "G_train, together with the train ground truth set of links (edge_ids_train, edge_labels_train), will be used for training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6iM7Sr6vQBb"
      },
      "source": [
        "### Creating the GCN link model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xbz2e3UvQBb"
      },
      "source": [
        "Next, we create the link generators for the train and test link examples to the model. The link generators take the pairs of nodes (`citing-paper`, `cited-paper`) that are given in the `.flow` method to the Keras model, together with the corresponding binary labels indicating whether those pairs represent true or false links.\n",
        "\n",
        "The number of epochs for training the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "geS2hjjPvQBc"
      },
      "outputs": [],
      "source": [
        "epochs = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezw5I30JvQBc"
      },
      "source": [
        "For training we create a generator on the `G_train` graph, and make an iterator over the training links using the generator's `flow()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JUBD8vCuvQBc",
        "outputId": "9f1d3989-4a1f-46f0-93b9-1388169a8789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GCN (local pooling) filters...\n"
          ]
        }
      ],
      "source": [
        "train_gen = FullBatchLinkGenerator(G_train, method=\"gcn\")\n",
        "train_flow = train_gen.flow(edge_ids_train, edge_labels_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "4KEWHBe2vQBc",
        "outputId": "7332ad4e-b08d-4855-e1dc-85c7b0ce8bde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GCN (local pooling) filters...\n"
          ]
        }
      ],
      "source": [
        "test_gen = FullBatchLinkGenerator(G_test, method=\"gcn\")\n",
        "test_flow = train_gen.flow(edge_ids_test, edge_labels_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gKRBC_VvQBc"
      },
      "source": [
        "Now we can specify our machine learning model, we need a few more parameters for this:\n",
        "\n",
        " * the `layer_sizes` is a list of hidden feature sizes of each layer in the model. In this example we use two GCN layers with 16-dimensional hidden node features at each layer.\n",
        " * `activations` is a list of activations applied to each layer's output\n",
        " * `dropout=0.3` specifies a 30% dropout at each layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhQkyws6vQBc"
      },
      "source": [
        "We create a GCN model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3Q3NW8-nvQBc"
      },
      "outputs": [],
      "source": [
        "gcn = GCN(\n",
        "    layer_sizes=[16, 16], activations=[\"relu\", \"relu\"], generator=train_gen, dropout=0.3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOAqJrBfvQBc"
      },
      "source": [
        "To create a Keras model we now expose the input and output tensors of the GCN model for link prediction, via the `GCN.in_out_tensors` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "P_fugyDpvQBc"
      },
      "outputs": [],
      "source": [
        "x_inp, x_out = gcn.in_out_tensors()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDbnI0gOvQBc"
      },
      "source": [
        "Final link classification layer that takes a pair of node embeddings produced by the GCN model, applies a binary operator to them to produce the corresponding link embedding (`ip` for inner product; other options for the binary operator can be seen by running a cell with `?LinkEmbedding` in it), and passes it through a dense layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "H3YheQhavQBc"
      },
      "outputs": [],
      "source": [
        "prediction = LinkEmbedding(activation=\"relu\", method=\"ip\")(x_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezTiUgrCvQBc"
      },
      "source": [
        "The predictions need to be reshaped from `(X, 1)` to `(X,)` to match the shape of the targets we have supplied above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "AUI_N3q1vQBd"
      },
      "outputs": [],
      "source": [
        "prediction = keras.layers.Reshape((-1,))(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX-WFFcPvQBd"
      },
      "source": [
        "Stack the GCN and prediction layers into a Keras model, and specify the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "hTP5LT4BvQBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777f7352-8cbb-4ca0-8d37-3f873f79e542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = keras.Model(inputs=x_inp, outputs=prediction)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(lr=0.01),\n",
        "    loss=keras.losses.binary_crossentropy,\n",
        "    metrics=[\"acc\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9yZ4eTIvQBd"
      },
      "source": [
        "Evaluate the initial (untrained) model on the train and test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "OHigVVUvvQBd",
        "outputId": "3f005bbb-cac7-4245-cc99-23d297d1704d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step - loss: 2.2215 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 2.2651 - acc: 0.0000e+00\n",
            "\n",
            "Train Set Metrics of the initial (untrained) model:\n",
            "\tloss: 2.2215\n",
            "\tacc: 0.0000\n",
            "\n",
            "Test Set Metrics of the initial (untrained) model:\n",
            "\tloss: 2.2651\n",
            "\tacc: 0.0000\n"
          ]
        }
      ],
      "source": [
        "init_train_metrics = model.evaluate(train_flow)\n",
        "init_test_metrics = model.evaluate(test_flow)\n",
        "\n",
        "print(\"\\nTrain Set Metrics of the initial (untrained) model:\")\n",
        "for name, val in zip(model.metrics_names, init_train_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
        "\n",
        "print(\"\\nTest Set Metrics of the initial (untrained) model:\")\n",
        "for name, val in zip(model.metrics_names, init_test_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kou8ws_JvQBd"
      },
      "source": [
        "Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rhDu1PdSvQBd",
        "outputId": "497f9b90-5a31-4b43-c62b-86affc002085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 - 2s - loss: 2.1276 - acc: 0.0000e+00 - val_loss: 0.7298 - val_acc: 0.0000e+00 - 2s/epoch - 2s/step\n",
            "Epoch 2/50\n",
            "1/1 - 0s - loss: 0.6981 - acc: 0.0000e+00 - val_loss: 5.5296 - val_acc: 0.0000e+00 - 90ms/epoch - 90ms/step\n",
            "Epoch 3/50\n",
            "1/1 - 0s - loss: 5.1424 - acc: 0.0000e+00 - val_loss: 4.7347 - val_acc: 0.0000e+00 - 85ms/epoch - 85ms/step\n",
            "Epoch 4/50\n",
            "1/1 - 0s - loss: 4.4849 - acc: 0.0000e+00 - val_loss: 1.4265 - val_acc: 0.0000e+00 - 89ms/epoch - 89ms/step\n",
            "Epoch 5/50\n",
            "1/1 - 0s - loss: 1.2056 - acc: 0.0000e+00 - val_loss: 0.6408 - val_acc: 0.0000e+00 - 85ms/epoch - 85ms/step\n",
            "Epoch 6/50\n",
            "1/1 - 0s - loss: 0.6207 - acc: 0.0000e+00 - val_loss: 0.7472 - val_acc: 0.0000e+00 - 93ms/epoch - 93ms/step\n",
            "Epoch 7/50\n",
            "1/1 - 0s - loss: 0.7290 - acc: 0.0000e+00 - val_loss: 0.7292 - val_acc: 0.0000e+00 - 88ms/epoch - 88ms/step\n",
            "Epoch 8/50\n",
            "1/1 - 0s - loss: 0.7059 - acc: 0.0000e+00 - val_loss: 0.6883 - val_acc: 0.0000e+00 - 106ms/epoch - 106ms/step\n",
            "Epoch 9/50\n",
            "1/1 - 0s - loss: 0.6983 - acc: 0.0000e+00 - val_loss: 0.8232 - val_acc: 0.0000e+00 - 115ms/epoch - 115ms/step\n",
            "Epoch 10/50\n",
            "1/1 - 0s - loss: 0.9285 - acc: 0.0000e+00 - val_loss: 0.7159 - val_acc: 0.0000e+00 - 87ms/epoch - 87ms/step\n",
            "Epoch 11/50\n",
            "1/1 - 0s - loss: 0.7309 - acc: 0.0000e+00 - val_loss: 0.6617 - val_acc: 0.0000e+00 - 83ms/epoch - 83ms/step\n",
            "Epoch 12/50\n",
            "1/1 - 0s - loss: 0.6298 - acc: 0.0000e+00 - val_loss: 0.6594 - val_acc: 0.0000e+00 - 88ms/epoch - 88ms/step\n",
            "Epoch 13/50\n",
            "1/1 - 0s - loss: 0.6128 - acc: 0.0000e+00 - val_loss: 0.6395 - val_acc: 0.0000e+00 - 84ms/epoch - 84ms/step\n",
            "Epoch 14/50\n",
            "1/1 - 0s - loss: 0.6137 - acc: 0.0000e+00 - val_loss: 0.6179 - val_acc: 0.0000e+00 - 90ms/epoch - 90ms/step\n",
            "Epoch 15/50\n",
            "1/1 - 0s - loss: 0.6098 - acc: 0.0000e+00 - val_loss: 0.6051 - val_acc: 0.0000e+00 - 96ms/epoch - 96ms/step\n",
            "Epoch 16/50\n",
            "1/1 - 0s - loss: 0.5828 - acc: 0.0000e+00 - val_loss: 0.6057 - val_acc: 0.0000e+00 - 95ms/epoch - 95ms/step\n",
            "Epoch 17/50\n",
            "1/1 - 0s - loss: 0.5926 - acc: 0.0000e+00 - val_loss: 0.6222 - val_acc: 0.0000e+00 - 96ms/epoch - 96ms/step\n",
            "Epoch 18/50\n",
            "1/1 - 0s - loss: 0.5726 - acc: 0.0000e+00 - val_loss: 0.6022 - val_acc: 0.0000e+00 - 89ms/epoch - 89ms/step\n",
            "Epoch 19/50\n",
            "1/1 - 0s - loss: 0.5210 - acc: 0.0000e+00 - val_loss: 0.5760 - val_acc: 0.0000e+00 - 89ms/epoch - 89ms/step\n",
            "Epoch 20/50\n",
            "1/1 - 0s - loss: 0.5576 - acc: 0.0000e+00 - val_loss: 0.5835 - val_acc: 0.0000e+00 - 90ms/epoch - 90ms/step\n",
            "Epoch 21/50\n",
            "1/1 - 0s - loss: 0.5300 - acc: 0.0000e+00 - val_loss: 0.5849 - val_acc: 0.0000e+00 - 87ms/epoch - 87ms/step\n",
            "Epoch 22/50\n",
            "1/1 - 0s - loss: 0.4650 - acc: 0.0000e+00 - val_loss: 0.5908 - val_acc: 0.0000e+00 - 98ms/epoch - 98ms/step\n",
            "Epoch 23/50\n",
            "1/1 - 0s - loss: 0.4792 - acc: 0.0000e+00 - val_loss: 0.5773 - val_acc: 0.0000e+00 - 95ms/epoch - 95ms/step\n",
            "Epoch 24/50\n",
            "1/1 - 0s - loss: 0.4921 - acc: 0.0000e+00 - val_loss: 0.5834 - val_acc: 0.0000e+00 - 88ms/epoch - 88ms/step\n",
            "Epoch 25/50\n",
            "1/1 - 0s - loss: 0.4894 - acc: 0.0000e+00 - val_loss: 0.6025 - val_acc: 0.0000e+00 - 86ms/epoch - 86ms/step\n",
            "Epoch 26/50\n",
            "1/1 - 0s - loss: 0.4443 - acc: 0.0000e+00 - val_loss: 0.5999 - val_acc: 0.0000e+00 - 90ms/epoch - 90ms/step\n",
            "Epoch 27/50\n",
            "1/1 - 0s - loss: 0.4793 - acc: 0.0000e+00 - val_loss: 0.5909 - val_acc: 0.0000e+00 - 86ms/epoch - 86ms/step\n",
            "Epoch 28/50\n",
            "1/1 - 0s - loss: 0.4228 - acc: 0.0000e+00 - val_loss: 0.5850 - val_acc: 0.0000e+00 - 84ms/epoch - 84ms/step\n",
            "Epoch 29/50\n",
            "1/1 - 0s - loss: 0.4040 - acc: 0.0000e+00 - val_loss: 0.5682 - val_acc: 0.0000e+00 - 103ms/epoch - 103ms/step\n",
            "Epoch 30/50\n",
            "1/1 - 0s - loss: 0.3849 - acc: 0.0000e+00 - val_loss: 0.5854 - val_acc: 0.0000e+00 - 89ms/epoch - 89ms/step\n",
            "Epoch 31/50\n",
            "1/1 - 0s - loss: 0.4182 - acc: 0.0000e+00 - val_loss: 0.5945 - val_acc: 0.0000e+00 - 103ms/epoch - 103ms/step\n",
            "Epoch 32/50\n",
            "1/1 - 0s - loss: 0.3960 - acc: 0.0000e+00 - val_loss: 0.5955 - val_acc: 0.0000e+00 - 95ms/epoch - 95ms/step\n",
            "Epoch 33/50\n",
            "1/1 - 0s - loss: 0.3911 - acc: 0.0000e+00 - val_loss: 0.6005 - val_acc: 0.0000e+00 - 85ms/epoch - 85ms/step\n",
            "Epoch 34/50\n",
            "1/1 - 0s - loss: 0.3631 - acc: 0.0000e+00 - val_loss: 0.6076 - val_acc: 0.0000e+00 - 93ms/epoch - 93ms/step\n",
            "Epoch 35/50\n",
            "1/1 - 0s - loss: 0.4261 - acc: 0.0000e+00 - val_loss: 0.6085 - val_acc: 0.0000e+00 - 88ms/epoch - 88ms/step\n",
            "Epoch 36/50\n",
            "1/1 - 0s - loss: 0.3706 - acc: 0.0000e+00 - val_loss: 0.6171 - val_acc: 0.0000e+00 - 87ms/epoch - 87ms/step\n",
            "Epoch 37/50\n",
            "1/1 - 0s - loss: 0.3615 - acc: 0.0000e+00 - val_loss: 0.5929 - val_acc: 0.0000e+00 - 89ms/epoch - 89ms/step\n",
            "Epoch 38/50\n",
            "1/1 - 0s - loss: 0.3456 - acc: 0.0000e+00 - val_loss: 0.5793 - val_acc: 0.0000e+00 - 100ms/epoch - 100ms/step\n",
            "Epoch 39/50\n",
            "1/1 - 0s - loss: 0.3140 - acc: 0.0000e+00 - val_loss: 0.5881 - val_acc: 0.0000e+00 - 88ms/epoch - 88ms/step\n",
            "Epoch 40/50\n",
            "1/1 - 0s - loss: 0.3200 - acc: 0.0000e+00 - val_loss: 0.5803 - val_acc: 0.0000e+00 - 102ms/epoch - 102ms/step\n",
            "Epoch 41/50\n",
            "1/1 - 0s - loss: 0.3229 - acc: 0.0000e+00 - val_loss: 0.5904 - val_acc: 0.0000e+00 - 92ms/epoch - 92ms/step\n",
            "Epoch 42/50\n",
            "1/1 - 0s - loss: 0.3321 - acc: 0.0000e+00 - val_loss: 0.6078 - val_acc: 0.0000e+00 - 102ms/epoch - 102ms/step\n",
            "Epoch 43/50\n",
            "1/1 - 0s - loss: 0.3093 - acc: 0.0000e+00 - val_loss: 0.6670 - val_acc: 0.0000e+00 - 85ms/epoch - 85ms/step\n",
            "Epoch 44/50\n",
            "1/1 - 0s - loss: 0.3170 - acc: 0.0000e+00 - val_loss: 0.6976 - val_acc: 0.0000e+00 - 85ms/epoch - 85ms/step\n",
            "Epoch 45/50\n",
            "1/1 - 0s - loss: 0.2864 - acc: 0.0000e+00 - val_loss: 0.6921 - val_acc: 0.0000e+00 - 86ms/epoch - 86ms/step\n",
            "Epoch 46/50\n",
            "1/1 - 0s - loss: 0.2887 - acc: 0.0000e+00 - val_loss: 0.6797 - val_acc: 0.0000e+00 - 87ms/epoch - 87ms/step\n",
            "Epoch 47/50\n",
            "1/1 - 0s - loss: 0.2626 - acc: 0.0000e+00 - val_loss: 0.6661 - val_acc: 0.0000e+00 - 94ms/epoch - 94ms/step\n",
            "Epoch 48/50\n",
            "1/1 - 0s - loss: 0.2666 - acc: 0.0000e+00 - val_loss: 0.6651 - val_acc: 0.0000e+00 - 110ms/epoch - 110ms/step\n",
            "Epoch 49/50\n",
            "1/1 - 0s - loss: 0.2768 - acc: 0.0000e+00 - val_loss: 0.6853 - val_acc: 0.0000e+00 - 85ms/epoch - 85ms/step\n",
            "Epoch 50/50\n",
            "1/1 - 0s - loss: 0.2579 - acc: 0.0000e+00 - val_loss: 0.6925 - val_acc: 0.0000e+00 - 91ms/epoch - 91ms/step\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_flow, epochs=epochs, validation_data=test_flow, verbose=2, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghz8B425vQBd"
      },
      "source": [
        "Plot the training history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "m5WyvPkjvQBd",
        "outputId": "3c11e8b4-50ff-4735-f0f1-6c58b1008128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x576 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"569.382437pt\" version=\"1.1\" viewBox=\"0 0 496.946344 569.382437\" width=\"496.946344pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 569.382437 \nL 496.946344 569.382437 \nL 496.946344 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 68.346344 260.25 \nL 489.746344 260.25 \nL 489.746344 7.2 \nL 68.346344 7.2 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pe4991859fc)\" d=\"M 87.500889 260.25 \nL 87.500889 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#pe4991859fc)\" d=\"M 165.682707 260.25 \nL 165.682707 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pe4991859fc)\" d=\"M 243.864526 260.25 \nL 243.864526 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#pe4991859fc)\" d=\"M 322.046344 260.25 \nL 322.046344 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pe4991859fc)\" d=\"M 400.228162 260.25 \nL 400.228162 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#pe4991859fc)\" d=\"M 478.40998 260.25 \nL 478.40998 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pe4991859fc)\" d=\"M 68.346344 225.743182 \nL 489.746344 225.743182 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.04 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(25.1365 229.922322)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#pe4991859fc)\" d=\"M 68.346344 179.734091 \nL 489.746344 179.734091 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.02 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(25.1365 183.913232)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pe4991859fc)\" d=\"M 68.346344 133.725 \nL 489.746344 133.725 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.00 -->\n      <g style=\"fill:#262626;\" transform=\"translate(34.354156 137.904141)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#pe4991859fc)\" d=\"M 68.346344 87.715909 \nL 489.746344 87.715909 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.02 -->\n      <g style=\"fill:#262626;\" transform=\"translate(34.354156 91.89505)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pe4991859fc)\" d=\"M 68.346344 41.706818 \nL 489.746344 41.706818 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.04 -->\n      <g style=\"fill:#262626;\" transform=\"translate(34.354156 45.885959)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- acc -->\n     <defs>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(18.14175 146.055)rotate(-90)scale(0.144 -0.144)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_12\">\n    <path clip-path=\"url(#pe4991859fc)\" d=\"M 87.500889 133.725 \nL 95.319071 133.725 \nL 103.137253 133.725 \nL 110.955435 133.725 \nL 118.773616 133.725 \nL 126.591798 133.725 \nL 134.40998 133.725 \nL 142.228162 133.725 \nL 150.046344 133.725 \nL 157.864526 133.725 \nL 165.682707 133.725 \nL 173.500889 133.725 \nL 181.319071 133.725 \nL 189.137253 133.725 \nL 196.955435 133.725 \nL 204.773616 133.725 \nL 212.591798 133.725 \nL 220.40998 133.725 \nL 228.228162 133.725 \nL 236.046344 133.725 \nL 243.864526 133.725 \nL 251.682707 133.725 \nL 259.500889 133.725 \nL 267.319071 133.725 \nL 275.137253 133.725 \nL 282.955435 133.725 \nL 290.773616 133.725 \nL 298.591798 133.725 \nL 306.40998 133.725 \nL 314.228162 133.725 \nL 322.046344 133.725 \nL 329.864526 133.725 \nL 337.682707 133.725 \nL 345.500889 133.725 \nL 353.319071 133.725 \nL 361.137253 133.725 \nL 368.955435 133.725 \nL 376.773616 133.725 \nL 384.591798 133.725 \nL 392.40998 133.725 \nL 400.228162 133.725 \nL 408.046344 133.725 \nL 415.864526 133.725 \nL 423.682707 133.725 \nL 431.500889 133.725 \nL 439.319071 133.725 \nL 447.137253 133.725 \nL 454.955435 133.725 \nL 462.773616 133.725 \nL 470.591798 133.725 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#pe4991859fc)\" d=\"M 87.500889 133.725 \nL 95.319071 133.725 \nL 103.137253 133.725 \nL 110.955435 133.725 \nL 118.773616 133.725 \nL 126.591798 133.725 \nL 134.40998 133.725 \nL 142.228162 133.725 \nL 150.046344 133.725 \nL 157.864526 133.725 \nL 165.682707 133.725 \nL 173.500889 133.725 \nL 181.319071 133.725 \nL 189.137253 133.725 \nL 196.955435 133.725 \nL 204.773616 133.725 \nL 212.591798 133.725 \nL 220.40998 133.725 \nL 228.228162 133.725 \nL 236.046344 133.725 \nL 243.864526 133.725 \nL 251.682707 133.725 \nL 259.500889 133.725 \nL 267.319071 133.725 \nL 275.137253 133.725 \nL 282.955435 133.725 \nL 290.773616 133.725 \nL 298.591798 133.725 \nL 306.40998 133.725 \nL 314.228162 133.725 \nL 322.046344 133.725 \nL 329.864526 133.725 \nL 337.682707 133.725 \nL 345.500889 133.725 \nL 353.319071 133.725 \nL 361.137253 133.725 \nL 368.955435 133.725 \nL 376.773616 133.725 \nL 384.591798 133.725 \nL 392.40998 133.725 \nL 400.228162 133.725 \nL 408.046344 133.725 \nL 415.864526 133.725 \nL 423.682707 133.725 \nL 431.500889 133.725 \nL 439.319071 133.725 \nL 447.137253 133.725 \nL 454.955435 133.725 \nL 462.773616 133.725 \nL 470.591798 133.725 \n\" style=\"fill:none;stroke:#dd8452;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 68.346344 260.25 \nL 68.346344 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 489.746344 260.25 \nL 489.746344 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 68.346344 260.25 \nL 489.746344 260.25 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 68.346344 7.2 \nL 489.746344 7.2 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 362.688844 60.993 \nL 479.666344 60.993 \nQ 482.546344 60.993 482.546344 58.113 \nL 482.546344 17.28 \nQ 482.546344 14.4 479.666344 14.4 \nL 362.688844 14.4 \nQ 359.808844 14.4 359.808844 17.28 \nL 359.808844 58.113 \nQ 359.808844 60.993 362.688844 60.993 \nz\n\" style=\"fill:#eaeaf2;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_14\">\n     <path d=\"M 365.568844 26.06175 \nL 394.368844 26.06175 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_15\"/>\n    <g id=\"text_7\">\n     <!-- train -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(405.888844 31.10175)scale(0.144 -0.144)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 365.568844 47.19825 \nL 394.368844 47.19825 \n\" style=\"fill:none;stroke:#dd8452;stroke-linecap:round;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_8\">\n     <!-- validation -->\n     <defs>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(405.888844 52.23825)scale(0.144 -0.144)\">\n      <use xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"176.025391\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"239.501953\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"300.78125\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"339.990234\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"367.773438\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"428.955078\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_8\">\n    <path d=\"M 68.346344 524.1 \nL 489.746344 524.1 \nL 489.746344 271.05 \nL 68.346344 271.05 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_7\">\n     <g id=\"line2d_18\">\n      <path clip-path=\"url(#p67a3235b2d)\" d=\"M 87.500889 524.1 \nL 87.500889 271.05 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(84.001514 541.958281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p67a3235b2d)\" d=\"M 165.682707 524.1 \nL 165.682707 271.05 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(158.683957 541.958281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_20\">\n      <path clip-path=\"url(#p67a3235b2d)\" d=\"M 243.864526 524.1 \nL 243.864526 271.05 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g style=\"fill:#262626;\" transform=\"translate(236.865776 541.958281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p67a3235b2d)\" d=\"M 322.046344 524.1 \nL 322.046344 271.05 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(315.047594 541.958281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_22\">\n      <path clip-path=\"url(#p67a3235b2d)\" d=\"M 400.228162 524.1 \nL 400.228162 271.05 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 40 -->\n      <g style=\"fill:#262626;\" transform=\"translate(393.229412 541.958281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p67a3235b2d)\" d=\"M 478.40998 524.1 \nL 478.40998 271.05 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_14\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(471.41123 541.958281)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- epoch -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(257.117844 559.187687)scale(0.144 -0.144)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_6\">\n     <g id=\"line2d_24\">\n      <path clip-path=\"url(#p67a3235b2d)\" d=\"M 68.346344 523.853119 \nL 489.746344 523.853119 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(51.847594 528.03226)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p67a3235b2d)\" d=\"M 68.346344 480.214824 \nL 489.746344 480.214824 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_17\">\n      <!-- 1 -->\n      <g style=\"fill:#262626;\" transform=\"translate(51.847594 484.393964)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_26\">\n      <path clip-path=\"url(#p67a3235b2d)\" d=\"M 68.346344 436.576529 \nL 489.746344 436.576529 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_18\">\n      <!-- 2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(51.847594 440.755669)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p67a3235b2d)\" d=\"M 68.346344 392.938233 \nL 489.746344 392.938233 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_19\">\n      <!-- 3 -->\n      <g style=\"fill:#262626;\" transform=\"translate(51.847594 397.117374)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_28\">\n      <path clip-path=\"url(#p67a3235b2d)\" d=\"M 68.346344 349.299938 \nL 489.746344 349.299938 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_20\">\n      <!-- 4 -->\n      <g style=\"fill:#262626;\" transform=\"translate(51.847594 353.479079)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p67a3235b2d)\" d=\"M 68.346344 305.661643 \nL 489.746344 305.661643 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_21\">\n      <!-- 5 -->\n      <g style=\"fill:#262626;\" transform=\"translate(51.847594 309.840783)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_22\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(44.852844 411.48225)rotate(-90)scale(0.144 -0.144)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_30\">\n    <path clip-path=\"url(#p67a3235b2d)\" d=\"M 87.500889 431.008326 \nL 95.319071 493.387523 \nL 103.137253 299.449078 \nL 110.955435 328.138605 \nL 118.773616 471.242514 \nL 126.591798 496.765609 \nL 134.40998 492.038751 \nL 142.228162 493.051 \nL 150.046344 493.381689 \nL 157.864526 483.333175 \nL 165.682707 491.957468 \nL 173.500889 496.370461 \nL 181.319071 497.110016 \nL 189.137253 497.07118 \nL 196.955435 497.242935 \nL 204.773616 498.419401 \nL 212.591798 497.995152 \nL 220.40998 498.864044 \nL 228.228162 501.115904 \nL 236.046344 499.522449 \nL 243.864526 500.725586 \nL 251.682707 503.55991 \nL 259.500889 502.943504 \nL 267.319071 502.377312 \nL 275.137253 502.495926 \nL 282.955435 504.4625 \nL 290.773616 502.936699 \nL 298.591798 505.404429 \nL 306.40998 506.221407 \nL 314.228162 507.056921 \nL 322.046344 505.603539 \nL 329.864526 506.570921 \nL 337.682707 506.788051 \nL 345.500889 508.008599 \nL 353.319071 505.258591 \nL 361.137253 507.681076 \nL 368.955435 508.076771 \nL 376.773616 508.773598 \nL 384.591798 510.148882 \nL 392.40998 509.890126 \nL 400.228162 509.760186 \nL 408.046344 509.359612 \nL 415.864526 510.356982 \nL 423.682707 510.021781 \nL 431.500889 511.35644 \nL 439.319071 511.255483 \nL 447.137253 512.395787 \nL 454.955435 512.220751 \nL 462.773616 511.775793 \nL 470.591798 512.597727 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path clip-path=\"url(#p67a3235b2d)\" d=\"M 87.500889 492.005197 \nL 95.319071 282.552273 \nL 103.137253 317.238 \nL 110.955435 461.601698 \nL 118.773616 495.890969 \nL 126.591798 491.244488 \nL 134.40998 492.032399 \nL 142.228162 493.815738 \nL 150.046344 487.929196 \nL 157.864526 492.611706 \nL 165.682707 494.979375 \nL 173.500889 495.078392 \nL 181.319071 495.947903 \nL 189.137253 496.889944 \nL 196.955435 497.448828 \nL 204.773616 497.419434 \nL 212.591798 496.70288 \nL 220.40998 497.573013 \nL 228.228162 498.717569 \nL 236.046344 498.388609 \nL 243.864526 498.328936 \nL 251.682707 498.072658 \nL 259.500889 498.659872 \nL 267.319071 498.393575 \nL 275.137253 497.562484 \nL 282.955435 497.674121 \nL 290.773616 498.06673 \nL 298.591798 498.323703 \nL 306.40998 499.057104 \nL 314.228162 498.30857 \nL 322.046344 497.91157 \nL 329.864526 497.864504 \nL 337.682707 497.646906 \nL 345.500889 497.338125 \nL 353.319071 497.30003 \nL 361.137253 496.922125 \nL 368.955435 497.978128 \nL 376.773616 498.574704 \nL 384.591798 498.190069 \nL 392.40998 498.531266 \nL 400.228162 498.090621 \nL 408.046344 497.331644 \nL 415.864526 494.748025 \nL 423.682707 493.409689 \nL 431.500889 493.648933 \nL 439.319071 494.190099 \nL 447.137253 494.786747 \nL 454.955435 494.83135 \nL 462.773616 493.946698 \nL 470.591798 493.635756 \n\" style=\"fill:none;stroke:#dd8452;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 68.346344 524.1 \nL 68.346344 271.05 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 489.746344 524.1 \nL 489.746344 271.05 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 68.346344 524.1 \nL 489.746344 524.1 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 68.346344 271.05 \nL 489.746344 271.05 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe4991859fc\">\n   <rect height=\"253.05\" width=\"421.4\" x=\"68.346344\" y=\"7.2\"/>\n  </clipPath>\n  <clipPath id=\"p67a3235b2d\">\n   <rect height=\"253.05\" width=\"421.4\" x=\"68.346344\" y=\"271.05\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "application/pdf": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sg.utils.plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vjyw2mtvQBd"
      },
      "source": [
        "Evaluate the trained model on test citation links:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ec5aG_JAvQBd",
        "outputId": "5412bd75-dec1-41b7-8531-1bd74d2dc764",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1918 - acc: 0.0000e+00\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.6925 - acc: 0.0000e+00\n",
            "\n",
            "Train Set Metrics of the trained model:\n",
            "\tloss: 0.1918\n",
            "\tacc: 0.0000\n",
            "\n",
            "Test Set Metrics of the trained model:\n",
            "\tloss: 0.6925\n",
            "\tacc: 0.0000\n"
          ]
        }
      ],
      "source": [
        "train_metrics = model.evaluate(train_flow)\n",
        "test_metrics = model.evaluate(test_flow)\n",
        "\n",
        "print(\"\\nTrain Set Metrics of the trained model:\")\n",
        "for name, val in zip(model.metrics_names, train_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
        "\n",
        "print(\"\\nTest Set Metrics of the trained model:\")\n",
        "for name, val in zip(model.metrics_names, test_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C - Graph Classification Problem"
      ],
      "metadata": {
        "id": "8Z9u-3dTna0Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-awurURnxIg"
      },
      "source": [
        "### Graph-level tasks: Graph classification\n",
        "\n",
        "Finally, in this part of the tutorial, we will have a closer look at how to apply GNNs to the task of graph classification. The goal is to classify an entire graph instead of single nodes or edges. Therefore, we are also given a dataset of multiple graphs that we need to classify based on some structural graph properties. The most common task for graph classification is molecular property prediction, in which molecules are represented as graphs. Each atom is linked to a node, and edges in the graph are the bonds between atoms. For example, look at the figure below. \n",
        "\n",
        "<center width=\"100%\"><img src=\"molecule_graph.svg\" width=\"600px\"></center>\n",
        "\n",
        "On the left, we have an arbitrary, small molecule with different atoms, whereas the right part of the image shows the graph representation. The atom types are abstracted as node features (e.g. a one-hot vector), and the different bond types are used as edge features. For simplicity, we will neglect the edge attributes in this tutorial, but you can include by using methods like the [Relational Graph Convolution](https://arxiv.org/abs/1703.06103) that uses a different weight matrix for each edge type.\n",
        "\n",
        "The dataset we will use below is called the MUTAG dataset. It is a common small benchmark for graph classification algorithms, and contain 188 graphs with 18 nodes and 20 edges on average for each graph. The graph nodes have 7 different labels/atom types, and the binary graph labels represent \"their mutagenic effect on a specific gram negative bacterium\" (the specific meaning of the labels are not too important here). The dataset is part of a large collection of different graph classification datasets, known as the [TUDatasets](https://chrsmrrs.github.io/datasets/), which is directly accessible via `torch_geometric.datasets.TUDataset` ([documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.TUDataset)) in PyTorch Geometric. We can load the dataset below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "MGbFAzYunxIg"
      },
      "outputs": [],
      "source": [
        "tu_dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"MUTAG\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mAn25binxIg"
      },
      "source": [
        "Let's look at some statistics for the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "nKfobY1CnxIg",
        "outputId": "eca79887-4211-4483-be3a-024d71955533",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data object: Data(x=[3371, 7], edge_index=[2, 7442], edge_attr=[7442, 4], y=[188])\n",
            "Length: 188\n",
            "Average label: 0.66\n"
          ]
        }
      ],
      "source": [
        "print(\"Data object:\", tu_dataset.data)\n",
        "print(\"Length:\", len(tu_dataset))\n",
        "print(f\"Average label: {tu_dataset.data.y.float().mean().item():4.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfvAvAGdnxIg"
      },
      "source": [
        "The first line shows how the dataset stores different graphs. The nodes, edges, and labels of each graph are concatenated to one tensor, and the dataset stores the indices where to split the tensors correspondingly. The length of the dataset is the number of graphs we have, and the \"average label\" denotes the percentage of the graph with label 1. As long as the percentage is in the range of 0.5, we have a relatively balanced dataset. It happens quite often that graph datasets are very imbalanced, hence checking the class balance is always a good thing to do.\n",
        "\n",
        "Next, we will split our dataset into a training and test part. Note that we do not use a validation set this time because of the small size of the dataset. Therefore, our model might overfit slightly on the validation set due to the noise of the evaluation, but we still get an estimate of the performance on untrained data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "vMFzoclenxIg"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "tu_dataset.shuffle()\n",
        "train_dataset = tu_dataset[:150]\n",
        "test_dataset = tu_dataset[150:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BILHqhRmnxIg"
      },
      "source": [
        "When using a data loader, we encounter a problem with batching $N$ graphs. Each graph in the batch can have a different number of nodes and edges, and hence we would require a lot of padding to obtain a single tensor. Torch geometric uses a different, more efficient approach: we can view the $N$ graphs in a batch as a single large graph with concatenated node and edge list. As there is no edge between the $N$ graphs, running GNN layers on the large graph gives us the same output as running the GNN on each graph separately. Visually, this batching strategy is visualized below (figure credit - PyTorch Geometric team, [tutorial here](https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=2owRWKcuoALo)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"torch_geometric_stacking_graphs.png\" width=\"600px\"></center>\n",
        "\n",
        "The adjacency matrix is zero for any nodes that come from two different graphs, and otherwise according to the adjacency matrix of the individual graph. Luckily, this strategy is already implemented in torch geometric, and hence we can use the corresponding data loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "wKZKkAkenxIg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c521bd6a-758f-4ded-e874-520f776b08bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ],
      "source": [
        "graph_train_loader = geom_data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "graph_val_loader = geom_data.DataLoader(test_dataset, batch_size=64) # Additional loader if you want to change to a larger dataset\n",
        "graph_test_loader = geom_data.DataLoader(test_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH9W8Np_nxIg"
      },
      "source": [
        "Let's load a batch below to see the batching in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ezgjBu3xnxIg",
        "outputId": "f9618064-165d-4703-deda-fd986dc0a8f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: DataBatch(edge_index=[2, 1512], x=[687, 7], edge_attr=[1512, 4], y=[38], batch=[687], ptr=[39])\n",
            "Labels: tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0])\n",
            "Batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(graph_test_loader))\n",
        "print(\"Batch:\", batch)\n",
        "print(\"Labels:\", batch.y[:10])\n",
        "print(\"Batch indices:\", batch.batch[:40])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GA2VxAnnxIh"
      },
      "source": [
        "We have 38 graphs stacked together for the test dataset. The batch indices, stored in `batch`, show that the first 12 nodes belong to the first graph, the next 22 to the second graph, and so on. These indices are important for performing the final prediction. To perform a prediction over a whole graph, we usually perform a pooling operation over all nodes after running the GNN model. In this case, we will use the average pooling. Hence, we need to know which nodes should be included in which average pool. Using this pooling, we can already create our graph network below. Specifically, we re-use our class `GNNModel` from before, and simply add an average pool and single linear layer for the graph prediction task. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "MkcVqch7nxIh"
      },
      "outputs": [],
      "source": [
        "class GraphGNNModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimension of input features\n",
        "            c_hidden - Dimension of hidden features\n",
        "            c_out - Dimension of output features (usually number of classes)\n",
        "            dp_rate_linear - Dropout rate before the linear layer (usually much higher than inside the GNN)\n",
        "            kwargs - Additional arguments for the GNNModel object\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.GNN = GNNModel(c_in=c_in, \n",
        "                            c_hidden=c_hidden, \n",
        "                            c_out=c_hidden, # Not our prediction output yet!\n",
        "                            **kwargs)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dp_rate_linear),\n",
        "            nn.Linear(c_hidden, c_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, batch_idx):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features per node\n",
        "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
        "            batch_idx - Index of batch element for each node\n",
        "        \"\"\"\n",
        "        x = self.GNN(x, edge_index)\n",
        "        x = geom_nn.global_mean_pool(x, batch_idx) # Average pooling\n",
        "        x = self.head(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGxU-ktEnxIh"
      },
      "source": [
        "Finally, we can create a PyTorch Lightning module to handle the training. It is similar to the modules we have seen before and does nothing surprising in terms of training. As we have a binary classification task, we use the Binary Cross Entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "dImimHl2nxIh"
      },
      "outputs": [],
      "source": [
        "class GraphLevelGNN(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, **model_kwargs):\n",
        "        super().__init__()\n",
        "        # Saving hyperparameters\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        self.model = GraphGNNModel(**model_kwargs)\n",
        "        self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, data, mode=\"train\"):\n",
        "        x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
        "        x = self.model(x, edge_index, batch_idx)\n",
        "        x = x.squeeze(dim=-1)\n",
        "        \n",
        "        if self.hparams.c_out == 1:\n",
        "            preds = (x > 0).float()\n",
        "            data.y = data.y.float()\n",
        "        else:\n",
        "            preds = x.argmax(dim=-1)\n",
        "        loss = self.loss_module(x, data.y)\n",
        "        acc = (preds == data.y).sum().float() / preds.shape[0]\n",
        "        return loss, acc\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=1e-2, weight_decay=0.0) # High lr because of small dataset and small model\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, acc = self.forward(batch, mode=\"train\")\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"val\")\n",
        "        self.log('val_acc', acc)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"test\")\n",
        "        self.log('test_acc', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSb5lkN1nxIi"
      },
      "source": [
        "Below we train the model on our dataset. It resembles the typical training functions we have seen so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "D1se9k2nnxIi"
      },
      "outputs": [],
      "source": [
        "def train_graph_classifier(model_name, **model_kwargs):\n",
        "    pl.seed_everything(42)\n",
        "    \n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         gpus=1 if str(device).startswith(\"cuda\") else 0,\n",
        "                         max_epochs=500,\n",
        "                         progress_bar_refresh_rate=0)\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"GraphLevel{model_name}.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything(42)\n",
        "        model = GraphLevelGNN(c_in=tu_dataset.num_node_features, \n",
        "                              c_out=1 if tu_dataset.num_classes==2 else tu_dataset.num_classes, \n",
        "                              **model_kwargs)\n",
        "        trainer.fit(model, graph_train_loader, graph_val_loader)\n",
        "        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "    # Test best model on validation and test set\n",
        "    train_result = trainer.test(model, graph_train_loader, verbose=False)\n",
        "    test_result = trainer.test(model, graph_test_loader, verbose=False)\n",
        "    result = {\"test\": test_result[0]['test_acc'], \"train\": train_result[0]['test_acc']} \n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-Y-704enxIi"
      },
      "source": [
        "Finally, let's perform the training and testing. Feel free to experiment with different GNN layers, hyperparameters, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "scrolled": true,
        "id": "8xG288L1nxIi",
        "outputId": "f97009e0-2180-4116-9d7f-bb0e7f03ee2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 42\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:97: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found pretrained model, loading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:490: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
            "  category=PossibleUserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  category=PossibleUserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:73: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        }
      ],
      "source": [
        "model, result = train_graph_classifier(model_name=\"GraphConv\", \n",
        "                                       c_hidden=256, \n",
        "                                       layer_name=\"GraphConv\", \n",
        "                                       num_layers=3, \n",
        "                                       dp_rate_linear=0.5,\n",
        "                                       dp_rate=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "yvP5j1GMnxIi",
        "outputId": "fde53300-b9be-4397-9189-52bc27086c4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train performance: 93.28%\n",
            "Test performance:  92.11%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train performance: {100.0*result['train']:4.2f}%\")\n",
        "print(f\"Test performance:  {100.0*result['test']:4.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moIp5hFNnxIi"
      },
      "source": [
        "The test performance shows that we obtain quite good scores on an unseen part of the dataset. It should be noted that as we have been using the test set for validation as well, we might have overfitted slightly to this set. Nevertheless, the experiment shows us that GNNs can be indeed powerful to predict the properties of graphs and/or molecules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ca1TH3VnxIi"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we have seen the application of neural networks to graph structures. We looked at how a graph can be represented (adjacency matrix or edge list), and discussed the implementation of common graph layers: GCN and GAT. The implementations showed the practical side of the layers, which is often easier than the theory. Finally, we experimented with different tasks, on node-, edge- and graph-level. Overall, we have seen that including graph information in the predictions can be crucial for achieving high performance. There are a lot of applications that benefit from GNNs, and the importance of these networks will likely increase over the next years."
      ]
    }
  ]
}